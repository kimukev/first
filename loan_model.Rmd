---
title: "Loan Prediction Project"
author: "Kevin Kimurgor"
date: "2025-07-15"
output: html_document
---

```{r}
## Data Source

The dataset was downloaded from Kaggle and contains features related to loan applications. The target variable is `loan_status`. Notably, the dataset has already been balanced using SMOTE, meaning the minority class has been synthetically oversampled. This helps mitigate the risk of model bias toward the majority class and allows more stable training.

```

```{r}
library(tidyverse)
library(caret)
library(car)
```

```{r}
loan <- read.csv("C:\\Users\\USER\\Downloads\\loan_data.csv")
```

```{r}
glimpse(loan)
dim(loan)

```

```{r}
#data cleaning
sum(duplicated(loan))
sum(is.na(loan))
```

```{r}
#convert all character data type to factors
loan <- loan |> 
  mutate(across(where(is.character),factor))
#convert loan_status to factor (the target variable)
loan$loan_status <- as.factor(loan$loan_status)
head(loan)

```

statistical models understand factors better e.g glm() ,lm ()

```{r}
levels(loan$loan_status)
```

```{r}
# recode factor variable loan_status i.e 1 = "approved" 0 = "rejected"
loan <- loan |>
  mutate(loan_status = dplyr::recode(loan_status,
                              "1" = "approved",
                              "0" = "rejected")) |>
  mutate(loan_status = factor(loan_status, levels = c("rejected", "approved")))

```

```{r}
#split the data into training and test data. implementing an 80 20 split
set.seed(1999)  #useful for reproducibility
index <- createDataPartition(loan$loan_status,p=0.8,list = FALSE)
training_data <- loan[index,]
test_data <- loan[-index,]
```

```{r}
ctrl <- trainControl(method = "cv",number = 10)
```

To ensure that the model generalizes well to unseen data, I implemented **10-fold cross-validation** during training. This technique splits the training set into 10 equal parts, using 9 parts for training and 1 part for validation in each iteration. The process is repeated 10 times, and the performance is averaged.

Cross-validation helps reduce the risk of overfitting and provides a more **reliable estimate** of model performance than a single train/test split. It also ensures that all training data points are used for both training and validation at different stages.

```{r}
#building a logistic regression model
model <- glm(loan_status ~ ., family= binomial,data = training_data)
vif(model)
```

Implemented variation inflation factor (VIF) to check for multicollinearity.

person's age and person's employment experience have moderate to high correlation with other predictors

```{r}
cor(training_data$person_age, training_data$person_emp_exp, use = "complete.obs")
```

A correlation of 0.955 is very strong this indicates extreme multicollinearity.

```{r}
model <- glm(loan_status~.-person_emp_exp,data = training_data,family = binomial)
```

I opted to remove person_emp_exp manually in the model.

```{r}
reduced_model <- step(model,direction = "backward")
```

## Feature Selection Using Backward Stepwise Selection

To improve model simplicity and interpretability, I applied **backward stepwise selection** using the step() function. This approach begins with the full model (excluded person_emp_exp) and iteratively removes the least useful variables â€” those that do not significantly contribute to model performance based on the **Akaike Information Criterion (AIC)**.

AIC balances model fit. it penalizes unnecessary predictors that do not improve the model meaningfully. The goal is to identify a model that explains the data well while avoiding overfitting.

Backward selection was preferred over forward selection in this case because it evaluates the full model first and eliminates predictors with minimal contribution.

```{r}
# Fit model using training data with selected features from the reduced model
final_model <- train(
  formula(reduced_model),
  data = training_data,
  method = "glm",
  family= binomial,
  trControl = ctrl
)

```

Using train enables me to apply 10 fold cross validation as set in ctrl. To also get results of the confusion matrix.

```{r}
summary(final_model)
```

```{r}
#Predicted Class ( "Approved"/"Rejected")
class_predictions <- predict(final_model,newdata = test_data,type = "raw")


```

type = "raw" returns class labels as approved rejected. Using `caret::predict()` on a classification model like logistic regression, applies a 0.5 threshold on probabilties by default to classify outcomes.

```{r}
 #Predicted Probabilities (for ROC)
prob_predictions <- predict(final_model, newdata = test_data, type = "prob")

#positive class is "Approved"
approved_probs <- prob_predictions$approved

```

```{r}
confusionMatrix(
  data = class_predictions,
  reference = test_data$loan_status,
  positive = "approved" 
)

```

### Model Evaluation

The final logistic regression model was evaluated using a hold-out test set. The confusion matrix showed an accuracy of **89.6%**, with a **sensitivity of 73.5%** and **specificity of 94.2%**, indicating that the model performs well in detecting both approved and rejected loans.

The **Kappa score of 0.69** suggests substantial agreement beyond chance, while the **balanced accuracy of 83.8%** further confirms that the model handles class imbalance reasonably well. Although the McNemar's test was significant (p \< 0.001), suggesting some asymmetry in errors, the overall model performance is acceptable for a baseline logistic regression.

```{r}
# Load pROC for ROC curve
library(pROC)

# Create ROC object
roc_obj <- roc(response = test_data$loan_status,
               predictor = approved_probs,
               levels = c("rejected", "approved"))  # Important: put positive class last

# Plot the ROC curve
plot(roc_obj, col = "blue", lwd = 2, main = "ROC Curve")
abline(a = 0, b = 1, lty = 2, col = "gray")  # Add diagonal reference line

# Print AUC
auc_value <- auc(roc_obj)
cat("AUC:", auc_value, "\n")

```

### ROC Curve and AUC

The ROC curve visualizes the model's performance across all classification thresholds.

(A perfect classifier leans towards the the top-left corner, while a random classifier would follow the diagonal line.)

In this project, the Area Under the Curve (AUC) was **0.954**, indicating strong classification ability. This means the logistic regression model is highly capable of distinguishing between approved and rejected loan applications.
